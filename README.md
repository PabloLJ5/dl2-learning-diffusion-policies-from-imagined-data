# Learning Diffusion Policies From Imagined Data
By Matthias Hagenauer, Piotr Sobecki, Tiberiu Iancu, Pablo Lozano Jimenez.

This project investigates whether synthetic demonstrations generated by DreMa—a compositional world model that creates diverse training scenes from limited data—can improve the training of diffusion policies for robotic manipulation. Diffusion policies, such as the 3D Diffusion Actor, learn multimodal action distributions via a denoising process and have shown strong performance in 3D scene-conditioned control tasks. These models typically rely on large real-world datasets, which are costly to collect. We attempt to bridge this gap by augmenting training data with DreMa-generated environments and evaluating whether this synthetic data can match or exceed the effectiveness of real demonstrations.


# Installation
The training and evaluation should be run in Apptainer (formerly Singularity) for stability across HPC systems. First, make sure to have `apptainer` installed.

```bash
> apptainer build --nv singularity.sif singularity.def
```
This results in a container file under `singularity.sif` that comes with all requirements (for training and online evaluation) preinstalled.

# Running

To train the model, see `scripts/train_keypose_drema.sh`. For evaluation, see `online_evaluation_rlbench/eval_drema.sh`. Note that these bash scripts should be run in apptainer:

```bash
> apptainer run --nv singularity.sif /bin/bash scripts/train_keypose_drema.sh
```

# Data Preparation

Make sure to obtain the relevant datasets from the [3D Diffuser Actor Repository](https://huggingface.co/katefgroup/3d_diffuser_actor/tree/main) and from the [DreMa repository](https://github.com/nickgkan/3d_diffuser_actor). Then see [Preparing RLBench dataset](./docs/DATA_PREPARATION_RLBENCH.md) and [Preparing CALVIN dataset](./docs/DATA_PREPARATION_CALVIN.md).


### (Optional) Encode language instructions

We provide our scripts for encoding language instructions with CLIP Text Encoder on CALVIN.  Otherwise, you can find the encoded instructions on CALVIN and RLBench ([Link](https://huggingface.co/katefgroup/3d_diffuser_actor/blob/main/instructions.zip)).
```
> python data_preprocessing/preprocess_calvin_instructions.py --output instructions/calvin_task_ABC_D/validation.pkl --model_max_length 16 --annotation_path ./calvin/dataset/task_ABC_D/validation/lang_annotations/auto_lang_ann.npy

> python data_preprocessing/preprocess_calvin_instructions.py --output instructions/calvin_task_ABC_D/training.pkl --model_max_length 16 --annotation_path ./calvin/dataset/task_ABC_D/training/lang_annotations/auto_lang_ann.npy
```

**Note:** We update our scripts for encoding language instructions on RLBench.
```
> python data_preprocessing/preprocess_rlbench_instructions.py  --tasks place_cups close_jar insert_onto_square_peg light_bulb_in meat_off_grill open_drawer place_shape_in_shape_sorter place_wine_at_rack_location push_buttons put_groceries_in_cupboard put_item_in_drawer put_money_in_safe reach_and_drag slide_block_to_color_target stack_blocks stack_cups sweep_to_dustpan_of_size turn_tap --output instructions.pkl
```

# License
This code base is released under the MIT License (refer to the LICENSE file for details).

# Acknowledgement
This codebase Has been adapted from [3D Diffusion Actor](https://github.com/nickgkan/3d_diffuser_actor).
